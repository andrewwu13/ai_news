[
  {
    "title": "Why tomorrow’s best devs won’t just code — they’ll curate, coordinate and command AI",
    "link": "https://venturebeat.com/programming-development/why-tomorrows-best-devs-wont-just-code-theyll-curate-coordinate-and-command-ai/",
    "author": "Roman Eloshvili, ComplyControl",
    "date": "August 3, 2025 1:05 PM",
    "img": "https://venturebeat.com/wp-content/uploads/2025/08/Junior-coders.jpeg?w=350&h=175&crop=1",
    "content": "As AI continues to take on more and more new competencies, junior coding, as we knew it, is rapidly becoming a thing of the past. Tasks that used to be the bread and butter for junior developers — such as repetitive scripting, HTML layout or simple DevOps setups — are now being reliably handled by AI assistants like ChatGPT, GitHub Copilot and Amazon CodeWhisperer.\nThis is not just an upgrade to speed and efficiency — we are looking at a serious structural change here. So where does that leave entry-level developers? And, speaking more broadly, where does it leave the software industry as a whole?\nFor decades, software engineering as a field had a fairly predictable pathway: Begin with the basics, build some landing pages, write test cases, troubleshoot minor bugs. As your skills grow, you can move toward architectural thinking and product ownership.\nBut now AI is vastly changing how the bottom end of that ladder operates, since it can do most junior-level tasks on its own.\nAs a result, beginners entering the industry are increasingly being asked to contribute at a level that used to require years of experience. It is not just about writing code anymore — it is about understanding systems, structuring problems and working alongside AI like a team member. That is a tall order. That said, I do believe that there is a way forward. It starts by changing the way we learn.\nIf you are just starting out, avoid relying on AI to get things done. It is tempting, sure, but in the long run, it is also harmful. If you skip the manual practice, you are missing out on building a deeper understanding of how software really works. That understanding is critical if you want to grow into the kind of developer who can lead, architect and guide AI instead of being replaced by it.\nThe way I see it, in the near future, the most valuable people in tech won’t be the ones who write perfect code. They will be those who know what should be built, why it matters and how to get an AI system to do most of the work cleanly and efficiently. In other words, the coder of tomorrow looks more like a product manager with solid technical expertise.\nBased on everything we covered above, I also feel the need to point out that it is not just individuals who need to rethink their roles. Entire teams are shifting. Where we once had clearly defined roles — front-end developer, back-end specialist, DevOps engineer, QA tester — we will soon see one developer managing a whole pipeline with the help of AI.\nAI-augmented developers will replace large teams that used to be necessary to move a project forward. In terms of efficiency, there is a lot to celebrate about this change — reduced communication time, faster results and higher bars for what one person can realistically accomplish.\nBut, of course, this does not mean teams will disappear altogether. It is just that the structure will change. Collaboration will focus more on strategic decisions, product alignment and making sure AI tools are being used responsibly and effectively. The human input will be less about implementation and more about direction.\nIf we look five to seven years ahead, I suspect that the idea of a “developer” as we know it today will have changed into something else entirely. We will likely see more hybrid roles — part developer, part designer, part product thinker. As already mentioned, the core part of the job won’t be to write code, but to shape ideas into working software using AI as your main creation tool. Or perhaps, even as a co-creator.\nBeing technically fluent will still remain a crucial requirement — but it won’t be enough to simply know how to code . You will need to understand product thinking, user needs and how to manage AI’s output. It will be more about system design and strategic vision.\nFor some, this may sound intimidating, but for others, it will also open many doors. People with creativity and a knack for problem-solving will have huge opportunities ahead of them.\nThe landscape is shifting, yes — there is no escaping that fact. But for those willing to adapt, one could argue it is shifting in their favor. The end of junior coding is not the end of learning. It is a sign that we need to reconsider what kind of talents we grow, how we structure teams and what makes someone a great developer.\nTo my mind, instead of mourning the loss of basic tasks, the industry as a whole should focus on building the skills that cannot be automated. At least, not yet. That means implementing a hybrid approach and learning how to work with AI as a partner rather than a competitor.\nRoman Eloshvili is founder of ComplyControl ."
  },
  {
    "title": "Why the AI era is forcing a redesign of the entire compute backbone",
    "link": "https://venturebeat.com/ai/why-the-ai-era-is-forcing-a-redesign-of-the-entire-compute-backbone/",
    "author": "Amin Vahdat, Google",
    "date": "August 3, 2025 11:05 AM",
    "img": "https://venturebeat.com/wp-content/uploads/2025/08/Rethinking-the-internet.webp?w=350&h=175&crop=1",
    "content": "The past few decades have seen almost unimaginable advances in compute performance and efficiency, enabled by Moore’s Law and underpinned by scale-out commodity hardware and loosely coupled software. This architecture has delivered online services to billions globally and put virtually all of human knowledge at our fingertips.\nBut the next computing revolution will demand much more. Fulfilling the promise of AI requires a step-change in capabilities far exceeding the advancements of the internet era. To achieve this, we as an industry must revisit some of the foundations that drove the previous transformation and innovate collectively to rethink the entire technology stack. Let’s explore the forces driving this upheaval and lay out what this architecture must look like.\nFor decades, the dominant trend in computing has been the democratization of compute through scale-out architectures built on nearly identical, commodity servers. This uniformity allowed for flexible workload placement and efficient resource utilization. The demands of gen AI , heavily reliant on predictable mathematical operations on massive datasets, are reversing this trend.\nWe are now witnessing a decisive shift towards specialized hardware — including ASICs, GPUs, and tensor processing units (TPUs) — that deliver orders of magnitude improvements in performance per dollar and per watt compared to general-purpose CPUs. This proliferation of domain-specific compute units, optimized for narrower tasks, will be critical to driving the continued rapid advances in AI.\nThese specialized systems will often require “all-to-all” communication, with terabit-per-second bandwidth and nanosecond latencies that approach local memory speeds. Today’s networks, largely based on commodity Ethernet switches and TCP/IP protocols, are ill-equipped to handle these extreme demands.\nAs a result, to scale gen AI workloads across vast clusters of specialized accelerators, we are seeing the rise of specialized interconnects, such as ICI for TPUs and NVLink for GPUs. These purpose-built networks prioritize direct memory-to-memory transfers and use dedicated hardware to speed information sharing among processors, effectively bypassing the overhead of traditional, layered networking stacks.\nThis move towards tightly integrated, compute-centric networking will be essential to overcoming communication bottlenecks and scaling the next generation of AI efficiently.\nFor decades, the performance gains in computation have outpaced the growth in memory bandwidth. While techniques like caching and stacked SRAM have partially mitigated this, the data-intensive nature of AI is only exacerbating the problem.\nThe insatiable need to feed increasingly powerful compute units has led to high bandwidth memory (HBM), which stacks DRAM directly on the processor package to boost bandwidth and reduce latency. However, even HBM faces fundamental limitations: The physical chip perimeter restricts total dataflow, and moving massive datasets at terabit speeds creates significant energy constraints.\nThese limitations highlight the critical need for higher-bandwidth connectivity and underscore the urgency for breakthroughs in processing and memory architecture. Without these innovations, our powerful compute resources will sit idle waiting for data, dramatically limiting efficiency and scale.\nToday’s advanced machine learning (ML) models often rely on carefully orchestrated calculations across tens to hundreds of thousands of identical compute elements, consuming immense power. This tight coupling and fine-grained synchronization at the microsecond level imposes new demands. Unlike systems that embrace heterogeneity, ML computations require homogeneous elements; mixing generations would bottleneck faster units. Communication pathways must also be pre-planned and highly efficient, since delays in a single element can stall an entire process.\nThese extreme demands for coordination and power are driving the need for unprecedented compute density. Minimizing the physical distance between processors becomes essential to reduce latency and power consumption, paving the way for a new class of ultra-dense AI systems .\nThis drive for extreme density and tightly coordinated computation fundamentally alters the optimal design for infrastructure, demanding a radical rethinking of physical layouts and dynamic power management to prevent performance bottlenecks and maximize efficiency.\nTraditional fault tolerance relies on redundancy among loosely connected systems to achieve high uptime. ML computing demands a different approach.\nFirst, the sheer scale of computation makes over-provisioning too costly. Second, model training is a tightly synchronized process, where a single failure can cascade to thousands of processors. Finally, advanced ML hardware often pushes to the boundary of current technology, potentially leading to higher failure rates.\nInstead, the emerging strategy involves frequent checkpointing — saving computation state — coupled with real-time monitoring, rapid allocation of spare resources and quick restarts. The underlying hardware and network design must enable swift failure detection and seamless component replacement to maintain performance.\nToday and looking forward, access to power is a key bottleneck for scaling AI compute . While traditional system design focuses on maximum performance per chip, we must shift to an end-to-end design focused on delivered, at-scale performance per watt. This approach is vital because it considers all system components — compute, network, memory, power delivery, cooling and fault tolerance — working together seamlessly to sustain performance. Optimizing components in isolation severely limits overall system efficiency.\nAs we push for greater performance, individual chips require more power, often exceeding the cooling capacity of traditional air-cooled data centers. This necessitates a shift towards more energy-intensive, but ultimately more efficient, liquid cooling solutions, and a fundamental redesign of data center cooling infrastructure.\nBeyond cooling, conventional redundant power sources, like dual utility feeds and diesel generators, create substantial financial costs and slow capacity delivery. Instead, we must combine diverse power sources and storage at multi-gigawatt scale, managed by real-time microgrid controllers. By leveraging AI workload flexibility and geographic distribution, we can deliver more capability without expensive backup systems needed only a few hours per year.\nThis evolving power model enables real-time response to power availability — from shutting down computations during shortages to advanced techniques like frequency scaling for workloads that can tolerate reduced performance. All of this requires real-time telemetry and actuation at levels not currently available.\nA critical lesson from the internet era is that security and privacy cannot be effectively bolted onto an existing architecture. Threats from bad actors will only grow more sophisticated, requiring protections for user data and proprietary intellectual property to be built into the fabric of the ML infrastructure. One important observation is that AI will, in the end, enhance attacker capabilities. This, in turn, means that we must ensure that AI simultaneously supercharges our defenses.\nThis includes end-to-end data encryption, robust data lineage tracking with verifiable access logs, hardware-enforced security boundaries to protect sensitive computations and sophisticated key management systems. Integrating these safeguards from the ground up will be essential for protecting users and maintaining their trust. Real-time monitoring of what will likely be petabits/sec of telemetry and logging will be key to identifying and neutralizing needle-in-the-haystack attack vectors, including those coming from insider threats.\nThe rhythm of hardware upgrades has shifted dramatically. Unlike the incremental rack-by-rack evolution of traditional infrastructure, deploying ML supercomputers requires a fundamentally different approach. This is because ML compute does not easily run on heterogeneous deployments; the compute code, algorithms and compiler must be specifically tuned to each new hardware generation to fully leverage its capabilities. The rate of innovation is also unprecedented, often delivering a factor of two or more in performance year over year from new hardware.\nTherefore, instead of incremental upgrades, a massive and simultaneous rollout of homogeneous hardware, often across entire data centers, is now required. With annual hardware refreshes delivering integer-factor performance improvements, the ability to rapidly stand up these colossal AI engines is paramount.\nThe goal must be to compress timelines from design to fully operational 100,000-plus chip deployments, enabling efficiency improvements while supporting algorithmic breakthroughs. This necessitates radical acceleration and automation of every stage, demanding a manufacturing-like model for these infrastructures. From architecture to monitoring and repair, every step must be streamlined and automated to leverage each hardware generation at unprecedented scale.\nThe rise of gen AI marks not just an evolution, but a revolution that requires a radical reimagining of our computing infrastructure. The challenges ahead — in specialized hardware, interconnected networks and sustainable operations — are significant, but so too is the transformative potential of the AI it will enable.\nIt is easy to see that our resulting compute infrastructure will be unrecognizable in the few years ahead, meaning that we cannot simply improve on the blueprints we have already designed. Instead, we must collectively, from research to industry, embark on an effort to re-examine the requirements of AI compute from first principles, building a new blueprint for the underlying global infrastructure. This in turn will result in fundamentally new capabilities, from medicine to education to business, at unprecedented scale and efficiency.\nAmin Vahdat is VP and GM for machine learning, systems and cloud AI at Google Cloud .\n"
  },
  {
    "title": "New vision model from Cohere runs on two GPUs, beats top-tier VLMs on visual tasks",
    "link": "https://venturebeat.com/ai/new-vision-model-from-cohere-runs-on-two-gpus-beats-top-tier-vlms-on-visual-tasks/",
    "author": "Emilia David",
    "date": "August 1, 2025 3:05 PM",
    "img": "https://venturebeat.com/wp-content/uploads/2024/10/Robot-poring-over-documents.jpg?w=350&h=175&crop=1",
    "content": "The rise in Deep Research features and other AI-powered analysis has given rise to more models and services looking to simplify that process and read more of the documents businesses actually use.\nCanadian AI company Cohere is banking on its models, including a newly released visual model, to make the case that Deep Research features should also be optimized for enterprise use cases.\nThe company has released Command A Vision, a visual model specifically targeting enterprise use cases, built on the back of its Command A model . The 112 billion parameter model can “unlock valuable insights from visual data, and make highly accurate, data-driven decisions through document optical character recognition (OCR) and image analysis,” the company says.\n“Whether it’s interpreting product manuals with complex diagrams or analyzing photographs of real-world scenes for risk detection, Command A Vision excels at tackling the most demanding enterprise vision challenges,” the company said in a blog post .\nThis means Command A Vision can read and analyze the most common types of images enterprises need: graphs, charts, diagrams, scanned documents and PDFs.\n? @cohere just dropped Command A Vision on @huggingface ? Designed for enterprise multimodal use cases: interpreting product manuals, analyzing photos, asking about charts… ❓?? A 112B dense vision-language model with SOTA performance – check out the benchmark metrics in… pic.twitter.com/ORMfM5f8cF\nSince it’s built on Command A’s architecture, Command A Vision requires two or fewer GPUs, just like the text model. The vision model also retains the text capabilities of Command A to read words on images and understands at least 23 languages. Cohere said that, unlike other models, Command A Vision reduces the total cost of ownership for enterprises and is fully optimized for retrieval use cases for businesses.\nCohere said it followed a Llava architecture to build its Command A models, including the visual model. This architecture turns visual features into soft vision tokens, which can be divided into different tiles.\nThese tiles are passed into the Command A text tower, “a dense, 111B parameters textual LLM,” the company said. “In this manner, a single image consumes up to 3,328 tokens.”\nCohere said it trained the visual model in three stages: vision-language alignment, supervised fine-tuning (SFT) and post-training reinforcement learning with human feedback (RLHF).\n“This approach enables the mapping of image encoder features to the language model embedding space,” the company said. “In contrast, during the SFT stage, we simultaneously trained the vision encoder, the vision adapter and the language model on a diverse set of instruction-following multimodal tasks.”\nBenchmark tests showed Command A Vision outperforming other models with similar visual capabilities.\nCohere pitted Command A Vision against OpenAI ’s GPT 4.1, Meta ’s Llama 4 Maverick, Mistral ’s Pixtral Large and Mistral Medium 3 in nine benchmark tests. The company did not mention if it tested the model against Mistral’s OCR-focused API, Mistral OCR .\nIt enables agents to securely see inside your organization’s visual data, unlocking the automation of tedious tasks involving slides, diagrams, PDFs, and photos. pic.twitter.com/iHZnUWekrk\nCommand A Vision outscored the other models in tests such as ChartQA, OCRBench, AI2D and TextVQA. Overall, Command A Vision had an average score of 83.1% compared to GPT 4.1’s 78.6%, Llama 4 Maverick’s 80.5% and the 78.3% from Mistral Medium 3.\nMost large language models (LLMs) these days are multimodal, meaning they can generate or understand visual media like photos or videos. However, enterprises generally use more graphical documents such as charts and PDFs, so extracting information from these unstructured data sources often proves difficult.\nWith Deep Research on the rise, the importance of bringing in models capable of reading, analyzing and even downloading unstructured data has grown.\nCohere also said it’s offering Command A Vision in an open weights system, in hopes that enterprises looking to move away from closed or proprietary models will start using its products. So far, there is some interest from developers.\nVery impressed at its accuracy extracting hand handwritten notes from an image!\nFinally, an AI that won’t judge my terrible doodles."
  },
  {
    "title": "Why open-source AI became an American national priority",
    "link": "https://venturebeat.com/ai/why-open-source-ai-became-an-american-national-priority/",
    "author": "Clément Delangue, Hugging Face CEO",
    "date": "August 1, 2025 12:07 PM",
    "img": "https://venturebeat.com/wp-content/uploads/2025/08/DDM-HF.webp?w=350&h=175&crop=1",
    "content": "When President Trump released the U.S. AI Action Plan last week, many were surprised to see “encourage open-source and open-weight AI,” as one of the administration’s top priorities. The White House has elevated what was once a highly technical topic into an urgent national concern — and a key strategy to winning the AI race against China.\nChina’s emphasis on open source, also highlighted in its own Action Plan released shortly after the U.S., makes the open-source race imperative. And the global soft power that comes with more open models from China makes their recent leadership even more notable.\nWhen DeepSeek-R1 , a powerful open-source large language model (LLM) out of China, was released earlier this year , it didn’t come with a press tour. No flashy demos. No keynote speeches. But it was open weights and open science. Open weight means anyone with the right skills and computing resources can run, replicate, or make a model their own; open science shares some of the tricks behind the model development.\nWithin hours, researchers and developers seized on it. Within days, it became the most-liked model of all time on Hugging Face — with thousands of variants created and used across major tech companies, research labs and startups. Most strikingly, this explosion of adoption happened not just abroad, but in the U.S. For the first time, American AI was being built on Chinese foundations.\nWithin a week, the U.S. stock market — sensing the tremor — took a tumble.\nIt turns out Deepseek was just the opening act. Dozens of Chinese research groups are now pushing the frontiers of open-source AI, sharing not only powerful models, but the data, code and scientific methods behind them. They’re moving quickly — and they’re doing it in the open.\nMeanwhile, U.S.-based companies — many of which pioneered the modern AI revolution — are increasingly closing up. Flagship models like GPT-4, Claude and Gemini are no longer released in ways that allow builders more control. They’re accessible only through chatbots or APIs: Gated interfaces that let you interact with a model but not see how it works, retrain it or use it freely. The model’s weights, training data and behavior remain proprietary, tightly controlled by a few tech giants.\nThis is a dramatic reversal. Between 2016 and 2020, the U.S. was the global leader in open-source AI. Research labs from Google, OpenAI, Stanford and elsewhere released breakthrough models and methods that laid the foundation for everything we now call “AI.” The transformer — the “T” in ChatGPT — was born out of this open culture. Hugging Face was created during this era to democratize access to these technologies.\nNow, the U.S. is slipping, and the implications are profound.\nAmerican scientists, startups and institutions are increasingly driven to build on Chinese open models because the best U.S. models are locked behind APIs. As each new open model emerges from abroad, Chinese companies like DeepSeek and Alibaba strengthen their positions as foundational layers in the global AI ecosystem. The tools that power America’s next generation of AI products, research and infrastructure are increasingly coming from overseas.\nAnd at a deeper level, there’s a more fundamental risk: Every advancement in AI — including the most closed systems — is built on open foundations. Proprietary models depend on open research, from transformer architecture to training libraries and evaluation frameworks. But more importantly, open-source increases a country’s velocity in building AI. It fuels rapid experimentation, lowers barriers to entry and creates compounding innovation.\nWhen openness slows down, the entire ecosystem follows. If the U.S. falls behind in open-source today, it may find itself falling behind in AI altogether.\nThis matters not just for innovation, but for security, science and democratic governance. Open models are transparent and auditable. They allow governments, educators, healthcare institutions and small businesses to adapt AI to their needs, without vendor lock-in or black-box dependencies.\nWe need more and better U.S.-developed open source models and artifacts. U.S. institutions already pushing for openness must build on their success. Meta’s open-weight Llama family has led to tens of thousands of variations on Hugging Face. The Allen Institute for AI continues to publish excellent fully open models. Promising startups like Black Forest are building open multimodal systems. Even OpenAI has suggested it may release open weights soon.\nWith more public and policy support for open-source AI, as demonstrated by the U.S. AI Action Plan, we can restart a decentralized movement that will ensure America’s leadership. It’s time for the American AI community to wake up, drop the “open is not safe” narrative, and return to its roots: Open science and open-source AI, powered by an unmatched community of frontier labs, big tech, startups, universities and non‑profits.\nWe can restart a decentralized movement that will ensure U.S. leadership, built on openness, competition and scientific inquiry, and empower the next generation of builders. If we want AI to reflect democratic principles, we have to build it in the open. And if the U.S. wants to lead the AI race, it must lead the open-source AI race.\nClément Delangue is the co-founder and CEO of Hugging Face ."
  },
  {
    "title": "Google releases Olympiad medal-winning Gemini 2.5 ‘Deep Think’ AI publicly — but there’s a catch…",
    "link": "https://venturebeat.com/ai/google-releases-olympiad-medal-winning-gemini-2-5-deep-think-ai-publicly-but-theres-a-catch/",
    "author": "Carl Franzen",
    "date": "August 1, 2025 8:39 AM",
    "img": "https://venturebeat.com/wp-content/uploads/2025/08/cfr0z3n_vibrant_bold_line_chunky_comic_book_expressionist_splas_a957df00-378f-4848-ac43-18736aa66f3e.png?w=350&h=175&crop=1",
    "content": "Google has officially launched Gemini 2.5 Deep Think, a new variation of its AI model engineered for deeper reasoning and complex problem-solving, which made headlines last month for winning a gold medal at the International Mathematical Olympiad (IMO) — the first time an AI model achieved the feat.\nHowever, this is unfortunately not the identical gold medal-winning model. It is in fact, a less powerful “bronze” version according to Google’s blog post and Logan Kilpatrick, Product Lead for Google AI Studio.\nAs Kilpatrick posted on the social network X : “This is a variation of our IMO gold model that is faster and more optimized for daily use. We are also giving the IMO gold full model to a set of mathematicians to test the value of the full capabilities.”\nNow available through the Gemini mobile app , this bronze model is accessible to subscribers of Google’s most expensive individual AI plan, AI Ultra , which costs $249.99 per month with a 3-month starting promotion at a reduced rate of $124.99/month for new subscribers.\nGoogle also said in its release blog post that it would bring Deep Think with and without tool usage integrations to “trusted testers” through the Gemini application programming interface (API) “in the coming weeks.”\nGemini 2.5 Deep Think builds on the Gemini family of large language models (LLMs), adding new capabilities aimed at reasoning through sophisticated problems.\nIt employs “parallel thinking” techniques to explore multiple ideas simultaneously and includes reinforcement learning to strengthen its step-by-step problem-solving ability over time.\nThe model is designed for use cases that benefit from extended deliberation, such as mathematical conjecture testing, scientific research, algorithm design, and creative iteration tasks like code and design refinement.\nEarly testers, including mathematicians such as Michel van Garrel, have used it to probe unsolved problems and generate potential proofs.\nAI power user and expert Ethan Mollick, a professor of the Wharton School of Business at the University of Pennsylvania, also posted on X that it was able to take a prompt he often uses to test the capabilities of new models — “create something I can paste into p5js that will startle me with its cleverness in creating something that invokes the control panel of a starship in the distant future” — and turned it into a 3D graphic, which is the first time any model has done that .\nHad early access to Gemini with Deep Think. Very good model, big gains over standard Gemini 2.5 Pro for a lot of problems. Here is the first attempt at the starship control panel prompt I try with every model. First time I have seen a model make a 3D interface in response. https://t.co/8iW2Pn6Xpu pic.twitter.com/bLFF2IcOP3\nGoogle highlights several key application areas for Deep Think:\nThe model also leads performance in benchmark evaluations such as LiveCodeBench V6 (for coding ability) and Humanity’s Last Exam (covering math, science, and reasoning).\nIt outscored Gemini 2.5 Pro and competing models like OpenAI’s GPT-4 and xAI’s Grok 4 by double digit margins on some categories (Reasoning & Knowledge, Code generation, and IMO 2025 Mathematics).\nWhile both Deep Think and Gemini 2.5 Pro are part of the Gemini 2.5 model family, Google positions Deep Think as a more capable and analytically skilled variant , particularly when it comes to complex reasoning and multi-step problem-solving.\nThis improvement stems from the use of parallel thinking and reinforcement learning techniques , which enable the model to simulate deeper cognitive deliberation.\nIn its official communication, Google describes Deep Think as better at handling nuanced prompts, exploring multiple hypotheses, and producing more refined outputs . This is supported by side-by-side comparisons in voxel art generation, where Deep Think adds more texture, structural fidelity, and compositional diversity than 2.5 Pro.\nThe improvements aren’t just visual or anecdotal. Google reports that Deep Think outperforms Gemini 2.5 Pro on multiple technical benchmarks related to reasoning, code generation, and cross-domain expertise. However, these gains come with tradeoffs in responsiveness and prompt acceptance.\nHere’s a breakdown:\nGoogle notes that Deep Think’s higher refusal rate is an area of active investigation. This may limit its flexibility in handling ambiguous or informal queries compared to 2.5 Pro. In contrast, 2.5 Pro remains better suited for users who prioritize speed and responsiveness , especially for lighter, general-purpose tasks.\nThis differentiation allows users to choose based on their priorities: 2.5 Pro for speed and fluidity , or Deep Think for rigor and reflection .\nIn July, Google DeepMind made headlines when a more advanced version of the Gemini Deep Think model achieved official gold-medal status at the 2025 IMO — the world’s most prestigious mathematics competition for high school students.\nThe system solved five of six challenging problems and became the first AI to receive gold-level scoring from the IMO.\nDemis Hassabis, CEO of Google DeepMind, announced the achievement on X, stating the model had solved problems end-to-end in natural language — without needing translation into formal programming syntax.\nThe IMO board confirmed the model scored 35 out of a possible 42 points, well above the gold threshold. Gemini 2.5 Deep Think’s solutions were described by competition president Gregor Dolinar as clear, precise, and in many cases, easier to follow than those of human competitors.\nHowever, the Gemini 2.5 Deep Think released to users is not that same competition model, rather, a lower performing but apparently faster version.\nGemini 2.5 Deep Think is available exclusively on the Google Gemini mobile app for iOS and Android at this time to users on the Google AI Ultra plan , part of the Google One subscription lineup, with pricing as follows.\nSubscribers can activate Deep Think in the Gemini app by selecting the 2.5 Pro model and toggling the “Deep Think” option.\nIt supports a fixed number of prompts per day and is integrated with capabilities like code execution and Google Search. The model also generates longer and more detailed outputs compared to standard versions.\nThe lower-tier Google AI Pro plan, priced at $19.99/month (with a free trial), does not include access to Deep Think, nor does the free Gemini AI service.\nGemini 2.5 Deep Think represents the practical application of a major research milestone.\nIt allows enterprises and organizations to tap into a Math Olympiad medal-winning model and have it join their staff, albeit only through an individual user account now.\nFor researchers receiving the full IMO-grade model, it offers a glimpse into the future of collaborative AI in mathematics. For Ultra subscribers, Deep Think provides a powerful step toward more capable and context-aware AI assistance, now running in the palm of their hand."
  }
]